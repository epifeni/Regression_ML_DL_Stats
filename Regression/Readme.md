<<<<<<< HEAD
# Linear Regression

Referece: [An Introduction to Statistical Learning Chapters 1 and 2](https://www.stat.berkeley.edu/~rabbee/s154/ISLR_First_Printing.pdf)  
=======
# Regression

Referece: [An Introduction to Statistical Learning](https://www.stat.berkeley.edu/~rabbee/s154/ISLR_First_Printing.pdf)<br>  
* Linear Regression - Chapters 1 and 2
* Logistic Regression - Chapters  4-4.3

## Bias-variance trade-off
* The point where we are just adding noise by adding more complixity (flexibility)
* The training error goes down but the test error will go up
* The model after the bias trade-off will begin to overfit<br>
<img width="434" height="416" alt="image" src="https://github.com/user-attachments/assets/16b80d59-ae0a-4f91-85ca-f5b9952f9394" /><br>

## Finding the right trade-off of model complexity
<img width="590" height="406" alt="image" src="https://github.com/user-attachments/assets/e633ba86-531e-4c0c-a348-dd29e4ba5185" /><br>






>>>>>>> 0f78a64d20740d4e22ca8c7bbf32a62f82dd1c0f




